{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf9783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date  year  month  day       author  \\\n",
      "0  2016-12-09 18:31:00  2016   12.0    9  Lee Drutman   \n",
      "1  2016-10-07 21:26:46  2016   10.0    7  Scott Davis   \n",
      "2  2018-01-26 00:00:00  2018    1.0   26          NaN   \n",
      "3  2019-06-27 00:00:00  2019    6.0   27          NaN   \n",
      "4  2016-01-27 00:00:00  2016    1.0   27          NaN   \n",
      "\n",
      "                                               title  \\\n",
      "0  We should take concerns about the health of li...   \n",
      "1  Colts GM Ryan Grigson says Andrew Luck's contr...   \n",
      "2       Trump denies report he ordered Mueller fired   \n",
      "3  France's Sarkozy reveals his 'Passions' but in...   \n",
      "4  Paris Hilton: Woman In Black For Uncle Monty's...   \n",
      "\n",
      "                                             article  \\\n",
      "0  This post is part of Polyarchy, an independent...   \n",
      "1   The Indianapolis Colts made Andrew Luck the h...   \n",
      "2  DAVOS, Switzerland (Reuters) - U.S. President ...   \n",
      "3  PARIS (Reuters) - Former French president Nico...   \n",
      "4  Paris Hilton arrived at LAX Wednesday dressed ...   \n",
      "\n",
      "                                                 url     section  \\\n",
      "0  https://www.vox.com/polyarchy/2016/12/9/138983...         NaN   \n",
      "1  https://www.businessinsider.com/colts-gm-ryan-...         NaN   \n",
      "2  https://www.reuters.com/article/us-davos-meeti...       Davos   \n",
      "3  https://www.reuters.com/article/france-politic...  World News   \n",
      "4  https://www.tmz.com/2016/01/27/paris-hilton-mo...         NaN   \n",
      "\n",
      "        publication  \n",
      "0               Vox  \n",
      "1  Business Insider  \n",
      "2           Reuters  \n",
      "3           Reuters  \n",
      "4               TMZ  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'all-the-news-2-1.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe4c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title       publication\n",
      "0  We should take concerns about the health of li...               Vox\n",
      "1  Colts GM Ryan Grigson says Andrew Luck's contr...  Business Insider\n",
      "2       Trump denies report he ordered Mueller fired           Reuters\n",
      "3  France's Sarkozy reveals his 'Passions' but in...           Reuters\n",
      "4  Paris Hilton: Woman In Black For Uncle Monty's...               TMZ\n",
      "title             37\n",
      "publication    12577\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select the relevant columns\n",
    "df = df[['title', 'publication']]\n",
    "\n",
    "# Display the first few rows of the selected columns\n",
    "print(df.head())\n",
    "\n",
    "# Check for any missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0426520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters               840094\n",
      "The New York Times    252259\n",
      "CNBC                  238096\n",
      "The Hill              208411\n",
      "People                136488\n",
      "CNN                   127602\n",
      "Refinery 29           111433\n",
      "Vice                  101137\n",
      "Mashable               94107\n",
      "Business Insider       57953\n",
      "The Verge              52424\n",
      "TechCrunch             52095\n",
      "TMZ                    49595\n",
      "Axios                  47815\n",
      "Vox                    47272\n",
      "Politico               46377\n",
      "Washington Post        40882\n",
      "Buzzfeed News          32819\n",
      "Gizmodo                27228\n",
      "Economist              26227\n",
      "Wired                  20243\n",
      "Fox News               20144\n",
      "Vice News              15539\n",
      "Hyperallergic          13551\n",
      "New Republic           11809\n",
      "New Yorker              4701\n",
      "Name: publication, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter out less frequent publications (if needed)\n",
    "# For example, keep only publications with more than 1000 articles\n",
    "df = df[df['publication'].isin(df['publication'].value_counts()[df['publication'].value_counts() > 1000].index)]\n",
    "\n",
    "# Display the remaining publications\n",
    "print(df['publication'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf67fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title'], df['publication'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ee3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title'], df['publication'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4517f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing titles\n",
    "df = df.dropna(subset=['title'])\n",
    "\n",
    "# Now, split the data into training and testing sets again\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title'], df['publication'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Proceed with TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca5d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fill missing titles with an empty string\n",
    "df['title'] = df['title'].fillna('')\n",
    "\n",
    "# Now, split the data into training and testing sets again\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title'], df['publication'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Proceed with TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72fe90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2f3a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3d0bfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Axios       0.44      0.01      0.01      9618\n",
      "  Business Insider       0.47      0.06      0.11     11476\n",
      "     Buzzfeed News       0.40      0.02      0.04      6441\n",
      "              CNBC       0.50      0.28      0.36     47635\n",
      "               CNN       0.27      0.10      0.15     25624\n",
      "         Economist       0.91      0.13      0.23      5230\n",
      "          Fox News       0.33      0.01      0.01      4025\n",
      "           Gizmodo       0.31      0.03      0.05      5389\n",
      "     Hyperallergic       0.62      0.09      0.16      2642\n",
      "          Mashable       0.34      0.27      0.30     18885\n",
      "      New Republic       0.50      0.00      0.00      2383\n",
      "        New Yorker       1.00      0.07      0.13       948\n",
      "            People       0.46      0.62      0.53     27327\n",
      "          Politico       0.44      0.03      0.05      9320\n",
      "       Refinery 29       0.58      0.45      0.50     22239\n",
      "           Reuters       0.66      0.91      0.77    167873\n",
      "               TMZ       0.61      0.30      0.40      9909\n",
      "        TechCrunch       0.79      0.88      0.83     10528\n",
      "          The Hill       0.55      0.94      0.70     41836\n",
      "The New York Times       0.36      0.55      0.44     50655\n",
      "         The Verge       0.42      0.32      0.36     10582\n",
      "              Vice       0.37      0.33      0.35     20155\n",
      "         Vice News       0.73      0.00      0.01      3029\n",
      "               Vox       0.64      0.11      0.18      9402\n",
      "   Washington Post       0.75      0.11      0.19      8102\n",
      "             Wired       0.35      0.01      0.03      4000\n",
      "\n",
      "          accuracy                           0.55    535253\n",
      "         macro avg       0.53      0.25      0.27    535253\n",
      "      weighted avg       0.53      0.55      0.49    535253\n",
      "\n",
      "[[    70     15      1    836    286      6      2     14      0    152\n",
      "       0      0    148     19     48   3676     18    205   2540   1270\n",
      "     115    150      0     27     10     10]\n",
      " [     4    688     11   1393    263      2      2      8      2    668\n",
      "       0      0   1058      5    620   2783    126    166    763   1837\n",
      "     477    566      0     20      9      5]\n",
      " [     3     29    123    276    581      1      2     12      1    425\n",
      "       0      0    684     15     84    669     51     37   1064   1687\n",
      "      66    581      0     42      7      1]\n",
      " [    14    189      6  13340    427      5      0     22      2    630\n",
      "       0      0    406     18    247  25488     95    352   2562   2734\n",
      "     538    483      0     53     17      7]\n",
      " [     7     53     18   1005   2582      2     15     30      4    577\n",
      "       2      0   2031    110    391   6068    166     44   5162   6454\n",
      "     170    615      0     77     36      5]\n",
      " [     0      3      1    325     50    696      0      4      5     38\n",
      "       0      0     46      2     31   1905      0     18    187   1764\n",
      "      21    116      0     16      1      1]\n",
      " [     1     10      3     79    296      1     29      9      1     47\n",
      "       0      0    692      1     45   1422     53      7    539    656\n",
      "      19    100      0      4     11      0]\n",
      " [     3     29      7    339     91      0      0    159      2    951\n",
      "       0      0     95      0     68    900      7     77    295    940\n",
      "     505    890      0     20      1     10]\n",
      " [     0      1      0     25     12      0      0      8    246     37\n",
      "       0      0     93      0     72    442      2      3     46   1420\n",
      "      13    219      0      0      3      0]\n",
      " [     1     82     16    732    282      0      1     52      8   5179\n",
      "       0      0   2117      7    750   2133    116    288    645   3409\n",
      "    1274   1737      0     40      5     11]\n",
      " [     0      1      4     68    196      3      0      1      0     60\n",
      "       5      0     81      5     28    261      7      1    419   1095\n",
      "       5    121      0     21      1      0]\n",
      " [     0      0      0     18      9      0      0      0      1     14\n",
      "       0     68     65      0     42    236      3      0     35    416\n",
      "       2     38      0      0      1      0]\n",
      " [     2     30      5    211    579      4      6      3      1    625\n",
      "       0      0  16925     16   2128   2094    444     16    688   2905\n",
      "      27    594      0     11     13      0]\n",
      " [     6      5      0    224    296      0      0      1      0     26\n",
      "       0      0    171    250     27   2133     15      8   4782   1309\n",
      "       3     38      0     21      5      0]\n",
      " [     2     44      1    280    186      2      0      1      4    471\n",
      "       0      0   3645      8   9903   4205    111     44    566   2011\n",
      "     113    612      0     24      6      0]\n",
      " [     5     29      4   3150    627      6      6     18      8    211\n",
      "       0      0   1009     18    253 153046    182    307   4036   4409\n",
      "     193    244      2      9     99      2]\n",
      " [     0      8      2     89    217      0      1      0      1    154\n",
      "       0      0   2947      1    295   1104   2970      1    402   1329\n",
      "      12    365      0      2      9      0]\n",
      " [     1     12      5    143     15      1      0      5      1     76\n",
      "       0      0     24      1     28    609      3   9300    101     99\n",
      "      28     73      0      3      0      0]\n",
      " [     0     12      5    233     24      2      7      4      2     52\n",
      "       0      0    293      0     61   1331     49     36  39338    259\n",
      "      47     71      0      3      4      3]\n",
      " [     6     58     14   1269    903     12      8     16     43    691\n",
      "       0      0   2143     47    919  12125    150     87   2632  27888\n",
      "     191   1342      2     57     41     11]\n",
      " [     4     47      1    656     74      2      0     47      4   1570\n",
      "       1      0    193      0    184   1580     18    445    286   1298\n",
      "    3339    780      0     34      2     17]\n",
      " [     2     49     36    587    350      4      0     55     57   1279\n",
      "       0      0   1239      3    542   2304    234     64    830   5588\n",
      "     285   6592      0     37     10      8]\n",
      " [     3      6     17    127    312      0      2      6      1     61\n",
      "       0      0     85      5      7    634     13      7    532    907\n",
      "      16    253     11     21      2      1]\n",
      " [    19     32     18    803    522     11      0     18      0    508\n",
      "       2      0    285     29    291    939     18    122   1656   2395\n",
      "     217    501      0   1005      5      6]\n",
      " [     2     25      4    248    254      0      8      2      5     80\n",
      "       0      0    362      7     62   3408     47      6    809   1714\n",
      "      17    155      0     11    876      0]\n",
      " [     4     19      2    314     31      3      0     26      0    615\n",
      "       0      0     60      0     82    549      4    112    177   1071\n",
      "     329    542      0      8      0     52]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4d1718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of y_test:\n",
      "1562316     Reuters\n",
      "891114          CNN\n",
      "1695417    The Hill\n",
      "1424086        CNBC\n",
      "1702704    The Hill\n",
      "Name: publication, dtype: object\n",
      "Sample of y_pred:\n",
      "['Reuters', 'The Hill', 'The Hill', 'Reuters', 'The Hill', 'Reuters', 'People', 'Reuters', 'TMZ', 'The New York Times']\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishchintamaneni/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/anishchintamaneni/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/anishchintamaneni/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Axios       0.00      0.00      0.00     47808\n",
      "  Business Insider       0.00      0.00      0.00     54710\n",
      "     Buzzfeed News       0.27      0.02      0.03     24360\n",
      "              CNBC       0.50      0.20      0.28    201532\n",
      "               CNN       0.26      0.04      0.08    104054\n",
      "         Economist       0.92      0.14      0.24     19408\n",
      "          Fox News       0.00      0.00      0.00     20088\n",
      "           Gizmodo       0.22      0.04      0.07     17473\n",
      "     Hyperallergic       0.84      0.03      0.06     10188\n",
      "          Mashable       0.21      0.31      0.25     60985\n",
      "      New Republic       0.27      0.00      0.00      6910\n",
      "        New Yorker       1.00      0.03      0.06      3468\n",
      "            People       0.46      0.58      0.51    103783\n",
      "          Politico       0.16      0.02      0.03     34143\n",
      "       Refinery 29       0.65      0.29      0.41     88168\n",
      "           Reuters       0.64      0.89      0.75    651806\n",
      "               TMZ       0.62      0.22      0.32     37513\n",
      "        TechCrunch       0.76      0.83      0.80     39197\n",
      "          The Hill       0.61      0.87      0.71    165675\n",
      "The New York Times       0.30      0.55      0.39    182242\n",
      "         The Verge       0.35      0.30      0.32     37186\n",
      "              Vice       0.22      0.48      0.30     61611\n",
      "         Vice News       0.16      0.00      0.01     10403\n",
      "               Vox       0.41      0.11      0.17     34066\n",
      "   Washington Post       0.00      0.00      0.00     40876\n",
      "             Wired       0.65      0.01      0.02     14133\n",
      "               nan       0.00      0.00      0.00     12577\n",
      "\n",
      "          accuracy                           0.51   2084363\n",
      "         macro avg       0.39      0.22      0.22   2084363\n",
      "      weighted avg       0.47      0.51      0.45   2084363\n",
      "\n",
      "Confusion Matrix:\n",
      "[[     0      0     29   2928    739     18      0     87      0   1458\n",
      "       2      0    806    333    133  18650     66    958   9289   9115\n",
      "     641   2294      8    238      0     16      0]\n",
      " [     0      0     33   5091    703     14      0    188      3   5930\n",
      "       0      0   4365    150   1433  13599    423   1017   2626   9631\n",
      "    2532   6626      6    338      0      2      0]\n",
      " [     0      0    370    673    629      1      0     45      2   3237\n",
      "       4      0   2310    112    110   2321     99    110   2562   6545\n",
      "     256   4607     24    341      0      2      0]\n",
      " [     0      0     27  40028    862     35      0    146      1   5019\n",
      "       0      0   1516    406    535 120059    243   1553   8632  13726\n",
      "    2546   5596      8    590      0      4      0]\n",
      " [     0      0    196   3727   4608      3      0    182      0   4724\n",
      "       5      0   7669    708    909  25721    435    233  14534  31355\n",
      "     937   7430     25    652      0      1      0]\n",
      " [     0      0      3   1141     82   2689      0     27      4    386\n",
      "       0      0    117     23     35   5010      5     67    424   7483\n",
      "      75   1710      1    124      0      2      0]\n",
      " [     0      0     51    267    843      3      0     52      1   1047\n",
      "       0      0   3174     40    122   6533    160     36   1714   4707\n",
      "      94   1193      6     44      0      1      0]\n",
      " [     0      0     30    667     99      1      0    698      1   3322\n",
      "       0      0    212      6     46   2870     28    308    760   2214\n",
      "    1766   4255      3    180      0      7      0]\n",
      " [     0      0      1     42     16      2      0     13    343    300\n",
      "       0      0    262      0     86   1293      8     24     83   4836\n",
      "      38   2836      0      5      0      0      0]\n",
      " [     0      0     31   2089    247      1      0    401      0  18728\n",
      "       2      0   4477     33    904   6435    161   1136   1265   7777\n",
      "    6534  10287      1    468      0      8      0]\n",
      " [     0      0     11    198    144      3      0      6      0    251\n",
      "      12      0    141     33     37    680      8      8   1044   3289\n",
      "      13    862      1    168      0      1      0]\n",
      " [     0      0      0     35     11      4      0      1      0     98\n",
      "       0    108    188      2     86    791      8      4     74   1516\n",
      "       5    530      0      6      0      1      0]\n",
      " [     0      0     53    557   1198      4      0     15      1   6904\n",
      "       1      0  60241     80   4583   7268   1137     50   1696  13321\n",
      "     227   6371      1     74      0      1      0]\n",
      " [     0      0     11    662    634      4      0      5      0    384\n",
      "       1      0    816    586     70   9336     53     22  13211   7491\n",
      "      22    743      1     91      0      0      0]\n",
      " [     0      0     13   1030    364     13      0     34      5   4791\n",
      "       1      0  14329     39  25916  21206    391    168   1482  10623\n",
      "     584   6911      0    268      0      0      0]\n",
      " [     0      0     58   8486   1671     40      0     78      3   3031\n",
      "       0      0   3510    212    427 582455    477   1343  13256  30960\n",
      "     901   4772     42     83      0      1      0]\n",
      " [     0      0     17    243    432      0      0      9      0   1928\n",
      "       0      0  11196     55    614   4035   8145      5    960   6244\n",
      "      36   3575      2     17      0      0      0]\n",
      " [     0      0     16    462     30      4      0     54      1    595\n",
      "       0      0     71     12     41   3331     12  32688    388    586\n",
      "     247    595      1     62      0      1      0]\n",
      " [     0      0    107   1145    493      9      0     65      4    967\n",
      "       1      0   2171      3    169   9973    220    198 143644   4561\n",
      "     185   1678      9     72      0      1      0]\n",
      " [     0      0     67   3401   1454     28      0    135     28   6057\n",
      "       0      0   6387    254   1602  35277    429    248   6572 101085\n",
      "     606  18123     15    468      0      6      0]\n",
      " [     0      0     12   1332     49      5      0    310      0   6509\n",
      "       0      0    369      9    186   5389     18   1828    760   3814\n",
      "   11315   5018      0    257      0      6      0]\n",
      " [     0      0     92   1102    353      5      0    314      6   6001\n",
      "       5      0   2600     28    884   5324    340    198   1937  11751\n",
      "     795  29527     13    334      0      2      0]\n",
      " [     0      0     70    367    377      0      0     36      0    611\n",
      "       2      0    373     41     14   1456     41     12   1436   3462\n",
      "      57   1867     36    144      0      1      0]\n",
      " [     0      0     52   1970    891     20      0     81      0   2583\n",
      "       3      0   1063    300    509   3952     51    364   5105   9080\n",
      "     547   3725     11   3755      0      4      0]\n",
      " [     0      0     32   1149    758      3      0     10      4    963\n",
      "       6      0   1769    165    122  16919    203     45   3467  12470\n",
      "     103   2529     12    147      0      0      0]\n",
      " [     0      0      2    774     37      3      0    209      0   2146\n",
      "       0      0    126      3    122   1771     12    403    500   2894\n",
      "    1403   3443      0    157      0    128      0]\n",
      " [     0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0  12577\n",
      "       0      0      0      0      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'all-the-news-2-1.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove rows with missing titles or dates\n",
    "df = df.dropna(subset=['title', 'date'])\n",
    "\n",
    "# Select relevant columns and sort by date\n",
    "df = df[['title', 'publication', 'date']]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(by='date')\n",
    "\n",
    "# Split the data into training and testing sets based on time\n",
    "split_date = '2017-01-01'  # Define your split date\n",
    "train_df = df[df['date'] < split_date]\n",
    "test_df = df[df['date'] >= split_date]\n",
    "\n",
    "# Separate the features and labels\n",
    "X_train = train_df['title']\n",
    "y_train = train_df['publication']\n",
    "X_test = test_df['title']\n",
    "y_test = test_df['publication']\n",
    "\n",
    "# Convert the text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Ensure that labels are strings and inspect them\n",
    "y_test = y_test.astype(str)\n",
    "y_pred = [str(label) for label in y_pred]\n",
    "\n",
    "# Print a sample of y_test and y_pred to debug\n",
    "print(\"Sample of y_test:\")\n",
    "print(y_test.head())\n",
    "print(\"Sample of y_pred:\")\n",
    "print(y_pred[:10])\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c48fa8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Axios       0.37      0.01      0.01      9424\n",
      "  Business Insider       0.48      0.06      0.11     11550\n",
      "     Buzzfeed News       0.54      0.02      0.04      6674\n",
      "              CNBC       0.50      0.28      0.36     47645\n",
      "               CNN       0.26      0.10      0.14     25570\n",
      "         Economist       0.93      0.13      0.23      5347\n",
      "          Fox News       0.39      0.01      0.02      4052\n",
      "           Gizmodo       0.32      0.03      0.05      5451\n",
      "     Hyperallergic       0.68      0.09      0.16      2737\n",
      "          Mashable       0.34      0.27      0.30     18805\n",
      "      New Republic       0.45      0.00      0.00      2412\n",
      "        New Yorker       0.99      0.07      0.14       956\n",
      "            People       0.46      0.62      0.53     27376\n",
      "          Politico       0.46      0.03      0.05      9118\n",
      "       Refinery 29       0.58      0.45      0.51     22382\n",
      "           Reuters       0.66      0.91      0.77    168231\n",
      "               TMZ       0.60      0.29      0.39     10076\n",
      "        TechCrunch       0.79      0.88      0.84     10483\n",
      "          The Hill       0.55      0.94      0.70     41608\n",
      "The New York Times       0.36      0.55      0.43     50261\n",
      "         The Verge       0.41      0.32      0.36     10474\n",
      "              Vice       0.37      0.33      0.35     20131\n",
      "         Vice News       0.70      0.00      0.00      3065\n",
      "               Vox       0.62      0.10      0.18      9299\n",
      "   Washington Post       0.75      0.10      0.18      8128\n",
      "             Wired       0.37      0.01      0.03      3998\n",
      "\n",
      "          accuracy                           0.55    535253\n",
      "         macro avg       0.54      0.25      0.26    535253\n",
      "      weighted avg       0.53      0.55      0.49    535253\n",
      "\n",
      "Confusion Matrix:\n",
      "[[    50     12      2    862    284      5      0     12      0    135\n",
      "       0      0    132     11     53   3531     24    195   2469   1314\n",
      "     130    152      0     30     15      6]\n",
      " [     2    709      3   1408    305      3      1     12      1    641\n",
      "       0      0    999     11    632   2762    136    182    792   1864\n",
      "     510    537      0     29      8      3]\n",
      " [     4     27    152    240    607      0      3      7      1    451\n",
      "       0      0    722      8     84    722     59     32   1136   1673\n",
      "      67    633      0     40      3      3]\n",
      " [    14    211      5  13277    415      5      1     21      0    618\n",
      "       0      0    420     23    253  25521     84    320   2545   2782\n",
      "     550    498      0     63     12      7]\n",
      " [     4     57     15   1010   2506      4     11     23      2    568\n",
      "       1      0   2074     87    412   6034    164     41   5185   6540\n",
      "     173    567      0     65     26      1]\n",
      " [     0      1      0    340     64    712      0      3      6     46\n",
      "       0      0     39      2     32   1949      0     21    181   1800\n",
      "      12    119      0     17      1      2]\n",
      " [     2      8      2     85    294      0     41     16      1     68\n",
      "       0      0    688      7     35   1396     54      8    530    681\n",
      "      24     96      0      3     12      1]\n",
      " [     5     22      3    347     88      0      3    144      0    953\n",
      "       0      0     95      0     64    860      6     66    310    984\n",
      "     573    903      0     16      0      9]\n",
      " [     0      0      0     34     13      0      0      5    243     44\n",
      "       0      0    108      0     80    459      5      3     51   1452\n",
      "      18    222      0      0      0      0]\n",
      " [     1     74     12    760    295      2      0     42      2   5118\n",
      "       0      0   2173      5    754   1974    132    296    599   3372\n",
      "    1290   1840      0     48      7      9]\n",
      " [     0      2      5     72    200      3      0      0      2     66\n",
      "       5      0     66      7     24    236      7      2    450   1140\n",
      "       4     97      0     24      0      0]\n",
      " [     0      1      0     15      6      0      0      1      2     14\n",
      "       0     70     62      0     46    224      1      1     37    423\n",
      "       1     48      0      2      1      1]\n",
      " [     1     19      2    175    632      0      3      1      0    701\n",
      "       0      0  16982     16   2021   2023    412     16    701   2960\n",
      "      36    650      0     12     13      0]\n",
      " [     5      5      0    208    257      0      1      0      0     18\n",
      "       0      0    167    234     38   2065     14      4   4809   1224\n",
      "       5     43      1     16      4      0]\n",
      " [     2     43      2    273    162      4      0      2      3    494\n",
      "       0      0   3636      4  10077   4160    130     44    537   1982\n",
      "     116    669      0     37      5      0]\n",
      " [     5     21      3   3284    636      3     12     15      6    223\n",
      "       1      0    934     25    232 153379    194    316   3869   4494\n",
      "     207    251      0     10    110      1]\n",
      " [     0     11      2    110    220      0      1      1      0    159\n",
      "       0      0   3020      0    327   1128   2880      2    403   1412\n",
      "       8    382      0      3      7      0]\n",
      " [     6      6      1    135     22      2      1      3      1    103\n",
      "       0      0     26      2     21    579      1   9274     95     99\n",
      "      33     65      1      7      0      0]\n",
      " [     0      5      2    225     33      1      8      3      4     52\n",
      "       0      0    309      0     58   1296     45     37  39152    253\n",
      "      32     85      0      1      4      3]\n",
      " [     8     55     16   1196    879      3      8      8     39    657\n",
      "       1      0   2070     39    915  12125    140     75   2668  27680\n",
      "     167   1402      0     66     33     11]\n",
      " [     4     35      5    654     83      0      0     32      4   1573\n",
      "       0      0    184      2    215   1551      5    434    292   1267\n",
      "    3329    759      0     29      4     13]\n",
      " [     3     51     22    550    325      3      0     56     36   1240\n",
      "       1      1   1206      2    572   2275    214     73    778   5758\n",
      "     301   6603      0     40     15      6]\n",
      " [     4      5     11    149    311      1      2      4      1     62\n",
      "       1      0    101      4      7    640     14      4    535    935\n",
      "      20    221      7     25      1      0]\n",
      " [    10     46     14    747    559      7      0     16      2    512\n",
      "       1      0    307     14    324    864     20    137   1669   2378\n",
      "     201    486      1    968      3     13]\n",
      " [     1     24      3    297    280      1      9      0      3     59\n",
      "       0      0    366      9     59   3403     63     10    816   1705\n",
      "      17    147      0      5    850      1]\n",
      " [     5     22      0    293     42      3      0     26      0    582\n",
      "       0      0     55      1     93    531      7    100    185   1068\n",
      "     374    542      0     16      0     53]]\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Axios       0.35      0.00      0.01      9600\n",
      "  Business Insider       0.48      0.06      0.11     11812\n",
      "     Buzzfeed News       0.42      0.02      0.04      6442\n",
      "              CNBC       0.50      0.28      0.36     47640\n",
      "               CNN       0.27      0.10      0.15     25463\n",
      "         Economist       0.90      0.13      0.23      5186\n",
      "          Fox News       0.33      0.01      0.02      3967\n",
      "           Gizmodo       0.29      0.03      0.05      5473\n",
      "     Hyperallergic       0.67      0.09      0.16      2696\n",
      "          Mashable       0.34      0.28      0.30     18703\n",
      "      New Republic       0.31      0.00      0.00      2298\n",
      "        New Yorker       1.00      0.08      0.14       985\n",
      "            People       0.46      0.61      0.52     27226\n",
      "          Politico       0.51      0.03      0.05      9377\n",
      "       Refinery 29       0.57      0.45      0.50     22075\n",
      "           Reuters       0.66      0.91      0.77    168520\n",
      "               TMZ       0.60      0.29      0.39      9851\n",
      "        TechCrunch       0.79      0.88      0.84     10460\n",
      "          The Hill       0.55      0.94      0.70     41644\n",
      "The New York Times       0.36      0.55      0.43     50408\n",
      "         The Verge       0.41      0.31      0.35     10464\n",
      "              Vice       0.37      0.32      0.35     20167\n",
      "         Vice News       0.79      0.00      0.01      3125\n",
      "               Vox       0.64      0.10      0.18      9545\n",
      "   Washington Post       0.77      0.11      0.19      8084\n",
      "             Wired       0.35      0.01      0.03      4042\n",
      "\n",
      "          accuracy                           0.55    535253\n",
      "         macro avg       0.53      0.25      0.26    535253\n",
      "      weighted avg       0.53      0.55      0.49    535253\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    47     12      1    816    301      5      0     10      0    152\n",
      "       0      0    121     18     49   3758     18    195   2453   1334\n",
      "     113    152      0     23      8     14]\n",
      " [     2    727     16   1455    282      4      0     20      0    680\n",
      "       0      0    987      4    669   2881    125    184    772   1923\n",
      "     513    517      0     28     16      7]\n",
      " [     5     22    150    248    576      0      1      9      3    413\n",
      "       0      0    672      8     87    723     47     20   1040   1690\n",
      "      76    615      0     28      8      1]\n",
      " [    15    222      3  13341    393     17      1     13      3    648\n",
      "       0      0    394     30    238  25445     95    314   2571   2795\n",
      "     532    486      0     63     12      9]\n",
      " [     4     56     28   1036   2607      3     14     36      1    574\n",
      "       4      0   2052     78    374   6118    163     43   5143   6237\n",
      "     163    626      1     65     34      3]\n",
      " [     1      6      0    309     54    699      0      3      2     49\n",
      "       0      0     40      2     30   1830      0     21    173   1803\n",
      "      18    127      0     17      2      0]\n",
      " [     0      7      3     69    265      0     33     11      0     74\n",
      "       0      0    705      2     45   1436     38      7    511    640\n",
      "      16     87      0      5     13      0]\n",
      " [     2     26     12    313     97      0      2    153      2    964\n",
      "       0      0    102      0     68    893     17     76    293    947\n",
      "     535    932      0     27      0     12]\n",
      " [     0      1      0     22     16      0      0      7    244     43\n",
      "       0      0     96      0     62    473      1      8     46   1452\n",
      "      10    211      0      0      3      1]\n",
      " [     2     69     17    740    307      2      1     54      1   5177\n",
      "       0      0   2087      6    786   2073    105    298    621   3396\n",
      "    1220   1681      0     45      3     12]\n",
      " [     0      0      3     71    179      1      0      1      0     54\n",
      "       5      0     71      2     31    230      7      1    438   1072\n",
      "       5    107      0     20      0      0]\n",
      " [     0      1      0     10      6      0      0      0      2     15\n",
      "       0     74     82      0     49    217      2      1     34    453\n",
      "       1     37      0      1      0      0]\n",
      " [     0     32      6    213    639      3      5      5      3    710\n",
      "       0      0  16743      9   2123   2080    450      7    698   2812\n",
      "      42    631      0      9      6      0]\n",
      " [     3      2      2    245    313      1      0      0      0     28\n",
      "       0      0    155    256     32   2126     14      3   4795   1316\n",
      "       6     43      0     29      8      0]\n",
      " [     1     49      3    260    171      2      0      1      0    465\n",
      "       0      0   3568      5   9887   4082    139     36    592   2006\n",
      "     106    669      0     31      2      0]\n",
      " [     4     22      6   3258    615      6     16     15      4    229\n",
      "       0      0    962     17    249 153540    194    319   4078   4436\n",
      "     196    254      1      6     91      2]\n",
      " [     0      9      2    116    251      0      0      1      3    165\n",
      "       0      0   2951      4    299   1123   2886      4    380   1303\n",
      "       7    340      0      0      7      0]\n",
      " [     4      3      3    124     13      1      0      5      2     72\n",
      "       0      0     15      0     27    662      2   9220    113     87\n",
      "      36     64      0      7      0      0]\n",
      " [     0      9      4    214     23      2      2      2      7     47\n",
      "       0      0    282      0     61   1344     37     25  39174    282\n",
      "      41     82      0      2      4      0]\n",
      " [     8     49     23   1241    901     13      5     15     34    671\n",
      "       0      0   2205     34    929  12224    146     78   2695  27529\n",
      "     167   1327      0     65     40      9]\n",
      " [     8     46      7    657     67      2      0     45      2   1591\n",
      "       1      0    198      0    199   1578     11    407    292   1244\n",
      "    3241    827      0     22      1     18]\n",
      " [     4     55     26    538    304      4      4     58     44   1243\n",
      "       2      0   1273      0    501   2308    235     79    842   5773\n",
      "     271   6554      2     30      7     10]\n",
      " [     4      3     21    117    318      2      2     13      0     75\n",
      "       0      0     87      2     14    634     20      4    536    986\n",
      "      18    228     15     23      0      3]\n",
      " [    15     37     19    794    560     10      1     10      0    509\n",
      "       3      0    299     20    330    958     18    160   1705   2349\n",
      "     212    524      0    998      5      9]\n",
      " [     1     21      1    250    268      2     13      1      4     61\n",
      "       1      0    350      4     87   3392     53      5    807   1700\n",
      "      29    138      0      7    889      0]\n",
      " [     6     28      0    325     45      2      0     35      1    647\n",
      "       0      0     61      0     82    522      3    106    205   1007\n",
      "     359    532      0     16      2     58]]\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Axios       0.43      0.01      0.01      9605\n",
      "  Business Insider       0.46      0.06      0.11     11488\n",
      "     Buzzfeed News       0.45      0.02      0.04      6504\n",
      "              CNBC       0.50      0.28      0.36     47694\n",
      "               CNN       0.27      0.10      0.15     25369\n",
      "         Economist       0.92      0.14      0.25      5241\n",
      "          Fox News       0.29      0.01      0.01      4060\n",
      "           Gizmodo       0.31      0.03      0.05      5480\n",
      "     Hyperallergic       0.66      0.09      0.15      2823\n",
      "          Mashable       0.34      0.28      0.30     18935\n",
      "      New Republic       0.25      0.00      0.00      2371\n",
      "        New Yorker       1.00      0.07      0.13       944\n",
      "            People       0.46      0.61      0.52     27336\n",
      "          Politico       0.45      0.03      0.05      9255\n",
      "       Refinery 29       0.58      0.45      0.51     22464\n",
      "           Reuters       0.66      0.91      0.77    167704\n",
      "               TMZ       0.60      0.29      0.39      9860\n",
      "        TechCrunch       0.79      0.88      0.83     10274\n",
      "          The Hill       0.55      0.94      0.70     41788\n",
      "The New York Times       0.36      0.55      0.43     50391\n",
      "         The Verge       0.42      0.32      0.36     10519\n",
      "              Vice       0.38      0.33      0.35     20278\n",
      "         Vice News       0.79      0.00      0.01      3124\n",
      "               Vox       0.62      0.10      0.18      9401\n",
      "   Washington Post       0.77      0.11      0.19      8207\n",
      "             Wired       0.50      0.02      0.04      4138\n",
      "\n",
      "          accuracy                           0.55    535253\n",
      "         macro avg       0.53      0.26      0.27    535253\n",
      "      weighted avg       0.53      0.55      0.49    535253\n",
      "\n",
      "Confusion Matrix:\n",
      "[[    63     14      4    837    285      3      0     14      0    140\n",
      "       0      0    128     20     58   3627     17    212   2564   1306\n",
      "     120    144      0     34     12      3]\n",
      " [     6    711      4   1391    276      4      1     13      2    710\n",
      "       0      0    958      7    614   2832    128    204    752   1844\n",
      "     462    528      0     23     10      8]\n",
      " [     1     21    129    269    582      1      5     11      0    479\n",
      "       0      0    679     12     92    701     51     32   1065   1681\n",
      "      82    570      1     34      6      0]\n",
      " [    14    217      1  13260    445      6      1     12      1    639\n",
      "       3      0    355     28    263  25495     86    327   2576   2845\n",
      "     525    516      0     60     13      6]\n",
      " [     2     56     22    960   2583      1     17     41      2    582\n",
      "       0      0   2032     88    359   6010    164     39   5101   6422\n",
      "     163    621      0     77     26      1]\n",
      " [     1      4      1    357     45    742      0      1      6     40\n",
      "       1      0     42      1     35   1816      3     12    196   1789\n",
      "      19    113      0     12      3      2]\n",
      " [     0      6      2     74    283      1     28      9      2     69\n",
      "       0      0    704      2     59   1434     51      3    540    657\n",
      "      21     94      0      5     16      0]\n",
      " [     4     20     11    341     98      1      0    162      2    979\n",
      "       0      0     82      0     44    917      9     79    273    993\n",
      "     575    864      1     18      1      6]\n",
      " [     0      4      0     26      9      1      0      6    247     45\n",
      "       0      0     96      0     83    478      3      4     53   1536\n",
      "      16    215      0      0      1      0]\n",
      " [     2     79     17    769    293      1      4     48      2   5215\n",
      "       1      0   2171      3    756   2078    131    288    582   3354\n",
      "    1244   1829      0     51      7     10]\n",
      " [     1      5      0     47    199      2      0      0      0     58\n",
      "       3      0     65      8     33    239      4      1    457   1103\n",
      "       2    118      0     25      1      0]\n",
      " [     0      0      0     14      6      0      0      0      0     16\n",
      "       0     67     72      0     39    242      1      1     33    414\n",
      "       2     36      0      1      0      0]\n",
      " [     2     29      6    177    626      2      7      1      2    679\n",
      "       0      0  16787     11   2126   2109    420     12    735   2876\n",
      "      35    670      0      9     15      0]\n",
      " [     4      7      2    208    313      1      1      2      0     25\n",
      "       0      0    190    249     27   2140     13      4   4758   1246\n",
      "       2     41      0     18      4      0]\n",
      " [     0     42      1    279    182      4      0      3      7    562\n",
      "       0      0   3620      9  10088   4071    118     45    587   2058\n",
      "     120    632      0     32      3      1]\n",
      " [     8     25      2   3284    560      3      4     13      5    207\n",
      "       0      0    970     22    244 152887    193    286   4003   4422\n",
      "     228    246      0      7     84      1]\n",
      " [     0     15      1    110    236      0      1      0      1    160\n",
      "       0      0   3034      1    269   1094   2871      5    354   1350\n",
      "      10    337      0      5      6      0]\n",
      " [     1      2      2    160      7      0      1      8      0     70\n",
      "       0      0     19      0     22    683      1   9011     91    101\n",
      "      28     61      0      6      0      0]\n",
      " [     3      8      1    218     21      2     11      5      3     48\n",
      "       0      0    313      0     54   1339     36     43  39311    267\n",
      "      43     53      0      2      5      2]\n",
      " [     3     63     12   1203    908     13      6     25     34    719\n",
      "       0      0   2095     46    943  12122    134     84   2633  27764\n",
      "     182   1295      0     64     34      9]\n",
      " [     4     51      6    598     69      3      0     39      5   1538\n",
      "       0      0    233      0    198   1589     10    428    289   1264\n",
      "    3405    747      0     33      0     10]\n",
      " [     4     41     32    500    336      3      1     56     44   1206\n",
      "       0      0   1247      2    499   2301    248     72    795   5845\n",
      "     256   6738      2     33      9      8]\n",
      " [     1      4     19    134    316      1      2      2      0     70\n",
      "       1      0     84      7      8    671     16      1    515    995\n",
      "      17    223     15     22      0      0]\n",
      " [    13     51     11    734    527     12      0     12      1    517\n",
      "       3      0    301     25    319    939     24    140   1683   2354\n",
      "     228    526      0    965      6     10]\n",
      " [     0     36      3    272    265      1      7      1      5     63\n",
      "       0      0    395      6     78   3405     47      5    854   1725\n",
      "      22    131      0      7    879      0]\n",
      " [     8     22      0    280     45      1      0     32      1    586\n",
      "       0      0     59      1    101    546      1    111    222   1109\n",
      "     370    545      0     19      3     76]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Axios       0.39      0.01      0.01      9508\n",
      "  Business Insider       0.49      0.06      0.11     11532\n",
      "     Buzzfeed News       0.37      0.02      0.03      6610\n",
      "              CNBC       0.50      0.28      0.36     47382\n",
      "               CNN       0.27      0.10      0.15     25621\n",
      "         Economist       0.91      0.13      0.23      5190\n",
      "          Fox News       0.25      0.00      0.01      4100\n",
      "           Gizmodo       0.30      0.03      0.05      5459\n",
      "     Hyperallergic       0.61      0.09      0.16      2640\n",
      "          Mashable       0.34      0.27      0.30     18871\n",
      "      New Republic       0.33      0.00      0.00      2353\n",
      "        New Yorker       0.98      0.06      0.12       912\n",
      "            People       0.46      0.62      0.53     27359\n",
      "          Politico       0.44      0.03      0.05      9368\n",
      "       Refinery 29       0.57      0.44      0.50     22413\n",
      "           Reuters       0.66      0.91      0.77    167859\n",
      "               TMZ       0.61      0.30      0.40      9862\n",
      "        TechCrunch       0.79      0.88      0.83     10472\n",
      "          The Hill       0.55      0.94      0.70     41731\n",
      "The New York Times       0.36      0.55      0.44     50509\n",
      "         The Verge       0.41      0.32      0.36     10415\n",
      "              Vice       0.37      0.33      0.35     20368\n",
      "         Vice News       0.67      0.00      0.01      3045\n",
      "               Vox       0.61      0.11      0.18      9407\n",
      "   Washington Post       0.75      0.10      0.18      8288\n",
      "             Wired       0.41      0.02      0.03      3979\n",
      "\n",
      "          accuracy                           0.55    535253\n",
      "         macro avg       0.52      0.25      0.26    535253\n",
      "      weighted avg       0.53      0.55      0.49    535253\n",
      "\n",
      "Confusion Matrix:\n",
      "[[    54     18      4    807    290      6      0     22      0    121\n",
      "       0      0    117     20     44   3620     22    193   2499   1347\n",
      "     143    128      0     32      8     13]\n",
      " [     5    727      8   1457    259      4      3     13      4    690\n",
      "       0      0   1009      1    609   2794    106    171    813   1841\n",
      "     445    529      0     33     10      1]\n",
      " [     5     24    114    257    574      1      3     15      2    457\n",
      "       0      0    706     10     91    685     42     46   1114   1740\n",
      "      87    585      0     45      4      3]\n",
      " [    17    209      6  13368    426      9      1     14      0    575\n",
      "       0      0    388     34    271  25288     75    360   2491   2778\n",
      "     543    449      2     57     17      4]\n",
      " [     6     53     21   1035   2547      1     10     39      3    574\n",
      "       2      0   2039     94    446   6088    165     42   5110   6441\n",
      "     184    594      0     88     34      5]\n",
      " [     3      2      0    321     76    692      0      1      4     42\n",
      "       0      0     40      6     34   1868      4     13    176   1748\n",
      "      21    119      0     14      2      4]\n",
      " [     3      4      6     74    309      0     18      8      2     84\n",
      "       0      0    710      2     52   1458     63      4    507    688\n",
      "      18     78      0      2      8      2]\n",
      " [     3     24     11    376    102      0      0    159      3    963\n",
      "       0      0    109      0     68    901      6     60    295    921\n",
      "     538    890      0     19      2      9]\n",
      " [     0      0      0     11     12      2      0      5    249     43\n",
      "       0      0    111      0     76    417      1      4     37   1408\n",
      "      18    245      0      1      0      0]\n",
      " [     3     79     20    762    285      4      0     45      2   5165\n",
      "       2      0   2171      7    745   2039    116    297    646   3341\n",
      "    1305   1774      0     53      4      6]\n",
      " [     0      1      1     65    178      3      0      0      1     68\n",
      "       5      0     82      8     27    222      4      0    441   1084\n",
      "       5    136      1     21      0      0]\n",
      " [     0      0      0     11      9      0      0      0      2     17\n",
      "       0     58     64      0     43    230      4      1     34    388\n",
      "       2     45      0      0      4      0]\n",
      " [     1     29     10    173    619      2     11      6      0    695\n",
      "       0      0  16894     16   2070   2031    413     10    755   2925\n",
      "      32    643      0     15      9      0]\n",
      " [     4      1      1    236    291      0      0      0      0     25\n",
      "       1      0    187    253     34   2155     18      4   4795   1284\n",
      "       0     47      0     24      8      0]\n",
      " [     3     53      2    293    197      1      0      3      7    511\n",
      "       0      0   3693      7   9902   4195    127     45    544   2037\n",
      "     119    633      0     36      4      1]\n",
      " [     2     23      5   3171    613      9     11     13      8    218\n",
      "       1      0    962     15    251 153142    168    297   3972   4417\n",
      "     199    261      0      6     93      2]\n",
      " [     0     13      1    106    179      0      3      1      3    167\n",
      "       0      0   2994      5    285   1098   2928      2    396   1295\n",
      "       6    373      0      0      7      0]\n",
      " [     2      5      4    166     11      0      0      7      3     74\n",
      "       0      0     17      1     23    653      2   9212    109     96\n",
      "      24     58      0      4      0      1]\n",
      " [     1      6      4    243     29      1      3      5      4     41\n",
      "       0      0    295      0     58   1325     67     36  39226    266\n",
      "      33     79      0      4      3      2]\n",
      " [     2     46     11   1228    884      2      3     14     59    700\n",
      "       1      1   2000     56    936  12056    161     81   2748  27736\n",
      "     196   1474      1     58     46      9]\n",
      " [     2     35      3    640     78      3      0     54      2   1607\n",
      "       0      0    185      0    186   1551      7    416    302   1229\n",
      "    3310    760      0     27      2     16]\n",
      " [     2     53     34    573    298      5      1     64     43   1303\n",
      "       3      0   1264      3    526   2358    240     61    776   5746\n",
      "     288   6663      1     37     14     12]\n",
      " [     2      6     15    106    288      2      0      8      0     58\n",
      "       0      0     98      5      8    642     21      4    570    925\n",
      "      13    238     10     22      2      2]\n",
      " [    14     30     21    737    563     10      0     10      0    486\n",
      "       0      0    310     20    324    916     16    152   1640   2401\n",
      "     210    529      0   1005      5      8]\n",
      " [     1     28      2    289    273      1      6      0      6     71\n",
      "       0      0    363      8     56   3544     58      7    784   1724\n",
      "      28    161      0     12    866      0]\n",
      " [     3     11      2    319     39      0      0     25      1    594\n",
      "       0      0     50      1     79    569      2     86    173   1034\n",
      "     341    559      0     21      1     69]]\n",
      "\n",
      "\n",
      "Fold 5\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Axios       0.29      0.00      0.01      9678\n",
      "  Business Insider       0.47      0.06      0.11     11571\n",
      "     Buzzfeed News       0.43      0.02      0.04      6589\n",
      "              CNBC       0.50      0.28      0.36     47732\n",
      "               CNN       0.26      0.10      0.14     25577\n",
      "         Economist       0.91      0.13      0.23      5263\n",
      "          Fox News       0.31      0.01      0.02      3965\n",
      "           Gizmodo       0.35      0.03      0.06      5365\n",
      "     Hyperallergic       0.67      0.09      0.16      2655\n",
      "          Mashable       0.34      0.28      0.30     18793\n",
      "      New Republic       0.50      0.00      0.00      2375\n",
      "        New Yorker       1.00      0.06      0.12       904\n",
      "            People       0.46      0.62      0.53     27190\n",
      "          Politico       0.48      0.03      0.05      9259\n",
      "       Refinery 29       0.57      0.45      0.50     22099\n",
      "           Reuters       0.66      0.91      0.77    167780\n",
      "               TMZ       0.60      0.29      0.39      9946\n",
      "        TechCrunch       0.79      0.88      0.83     10406\n",
      "          The Hill       0.55      0.94      0.69     41640\n",
      "The New York Times       0.36      0.55      0.44     50659\n",
      "         The Verge       0.41      0.31      0.36     10552\n",
      "              Vice       0.37      0.33      0.35     20193\n",
      "         Vice News       0.71      0.00      0.01      3180\n",
      "               Vox       0.66      0.11      0.18      9620\n",
      "   Washington Post       0.77      0.11      0.19      8175\n",
      "             Wired       0.38      0.01      0.03      4086\n",
      "\n",
      "          accuracy                           0.55    535252\n",
      "         macro avg       0.53      0.25      0.26    535252\n",
      "      weighted avg       0.53      0.55      0.49    535252\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    43     11      3    861    310      5      3     19      0    132\n",
      "       0      0    136     18     63   3714     17    180   2555   1304\n",
      "     137    128      0     25      6      8]\n",
      " [     6    687      8   1395    282      6      0     14      1    719\n",
      "       0      0    989      3    621   2792    148    201    784   1886\n",
      "     458    537      0     17     10      7]\n",
      " [     2     31    143    252    588      2      6      8      0    451\n",
      "       0      0    724     14     66    689     48     37    995   1789\n",
      "      73    620      1     42      7      1]\n",
      " [    18    208      7  13323    426      7      1     16      0    670\n",
      "       0      0    415     29    259  25390    104    372   2547   2860\n",
      "     530    475      0     53     17      5]\n",
      " [     8     62     16   1031   2514      3     16     39      1    580\n",
      "       3      0   2020     79    405   6071    159     35   5193   6516\n",
      "     151    584      0     69     18      4]\n",
      " [     0      5      0    294     58    700      0      1      4     39\n",
      "       0      0     50      2     30   1933      2     21    168   1787\n",
      "      18    138      1      9      2      1]\n",
      " [     0      4      6     64    294      1     31      6      1     72\n",
      "       0      0    703      3     40   1323     63      3    533    700\n",
      "      19     84      0      0     14      1]\n",
      " [     4     30      5    374    113      1      1    176      2    933\n",
      "       0      0     83      0     52    871     10     68    282    978\n",
      "     509    845      0     21      1      6]\n",
      " [     0      1      0     31     13      1      0      0    247     31\n",
      "       0      0    101      0     66    454      2      7     43   1437\n",
      "       8    210      0      0      3      0]\n",
      " [     7     71     19    753    269      1      0     39      9   5174\n",
      "       0      0   2123      8    765   2082     92    279    626   3424\n",
      "    1236   1753      0     47      7      9]\n",
      " [     1      2      0     83    192      4      0      0      0     45\n",
      "       4      0     57      7     30    231     10      1    454   1104\n",
      "       4    128      0     17      0      1]\n",
      " [     0      1      0     17      7      0      0      0      3     11\n",
      "       0     56     51      0     46    209      4      0     34    414\n",
      "       1     45      0      3      0      2]\n",
      " [     2     38      8    213    589      1      5      3      3    632\n",
      "       0      0  16875      3   2088   2157    415     13    712   2767\n",
      "      42    598      0      9     17      0]\n",
      " [     8      2      3    213    310      1      1      1      0     24\n",
      "       0      0    194    245     31   2042     16      6   4849   1252\n",
      "       0     37      0     23      1      0]\n",
      " [     3     61      4    302    168      1      0      2      5    475\n",
      "       0      0   3594      5   9949   4090    129     42    525   2011\n",
      "     108    595      0     25      5      0]\n",
      " [     8     23      5   3160    627      4      2     11      6    215\n",
      "       0      0    974     22    239 152994    195    265   4096   4413\n",
      "     214    218      0      8     79      2]\n",
      " [     0      8      4    102    219      0      4      1      0    147\n",
      "       0      0   3058      2    294   1112   2859      1    409   1367\n",
      "       7    341      0      2      9      0]\n",
      " [     2      5      3    159     15      2      0      4      0     93\n",
      "       0      0     26      0     35    593      6   9144    106    104\n",
      "      33     68      0      6      0      2]\n",
      " [     3      9      2    230     25      3      9      5      0     33\n",
      "       0      0    299      0     66   1344     42     28  39207    252\n",
      "      26     51      0      0      4      2]\n",
      " [     5     41      7   1200    927      8      8     17     47    657\n",
      "       0      0   2112     37    945  12158    166     80   2688  27942\n",
      "     203   1301      1     53     51      5]\n",
      " [     4     48      7    635     83      1      1     31      3   1564\n",
      "       0      0    190      0    209   1608     13    467    286   1286\n",
      "    3296    762      0     41      2     15]\n",
      " [     2     42     35    563    351      6      3     64     30   1320\n",
      "       0      0   1221      4    525   2359    208     73    796   5664\n",
      "     308   6567      1     28     15      8]\n",
      " [     2      5     20    147    329      0      1      4      0     56\n",
      "       0      0     94      7      7    631     20      4    599    958\n",
      "      17    252     10     14      3      0]\n",
      " [    13     42     23    734    547     10      0     13      0    534\n",
      "       1      0    295     17    314    957     20    141   1795   2411\n",
      "     205    524      0   1013      4      7]\n",
      " [     2     26      1    284    277      1      9      1      4     74\n",
      "       0      0    355      6     75   3476     53      9    747   1690\n",
      "      16    155      0      5    909      0]\n",
      " [     5     11      0    313     48      2      0     22      1    634\n",
      "       0      0     79      0     94    533      3    114    191   1064\n",
      "     343    560      0     15      1     53]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'all-the-news-2-1.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove rows with missing titles or dates\n",
    "df = df.dropna(subset=['title', 'publication'])\n",
    "\n",
    "# Ensure that 'date' column is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Handle invalid parsing\n",
    "\n",
    "# Drop rows where 'date' is NaT after conversion\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Select relevant columns and sort by date\n",
    "df = df[['title', 'publication', 'date']]\n",
    "df = df.sort_values(by='date')\n",
    "\n",
    "# Prepare the pipeline\n",
    "pipeline = make_pipeline(TfidfVectorizer(max_features=5000), MultinomialNB())\n",
    "\n",
    "# Initialize K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(df)):\n",
    "    X_train, X_test = df['title'].iloc[train_index], df['title'].iloc[test_index]\n",
    "    y_train, y_test = df['publication'].iloc[train_index], df['publication'].iloc[test_index]\n",
    "    \n",
    "    # Convert labels to strings to avoid type issues\n",
    "    y_train = y_train.astype(str)\n",
    "    y_test = y_test.astype(str)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(f\"Fold {fold+1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'all-the-news-2-1.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove rows with missing titles or publications\n",
    "df = df.dropna(subset=['title', 'publication'])\n",
    "\n",
    "# Ensure that 'date' column is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Handle invalid parsing\n",
    "\n",
    "# Drop rows where 'date' is NaT after conversion\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Select relevant columns and sort by date\n",
    "df = df[['title', 'publication', 'date']]\n",
    "df = df.sort_values(by='date')\n",
    "\n",
    "# Prepare the pipeline\n",
    "pipeline = make_pipeline(TfidfVectorizer(max_features=5000), MultinomialNB())\n",
    "\n",
    "# Initialize Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Initialize lists to store results\n",
    "reports = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Perform Leave-One-Out Cross-Validation\n",
    "for train_index, test_index in loo.split(df):\n",
    "    X_train, X_test = df['title'].iloc[train_index], df['title'].iloc[test_index]\n",
    "    y_train, y_test = df['publication'].iloc[train_index], df['publication'].iloc[test_index]\n",
    "    \n",
    "    # Convert labels to strings to avoid type issues\n",
    "    y_train = y_train.astype(str)\n",
    "    y_test = y_test.astype(str)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Store the results for this fold\n",
    "    reports.append(classification_report(y_test, y_pred, output_dict=True, zero_division=0))\n",
    "    confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Aggregate and print results\n",
    "print(\"Leave-One-Out Cross-Validation Results:\")\n",
    "for i, (report, cm) in enumerate(zip(reports, confusion_matrices), start=1):\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df515a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
